{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rL1bJayNGtY2"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Path to the folder containing CSV files\n",
        "folder_path = '/content/drive/My Drive/Preprocessed'\n",
        "\n",
        "# Combine all CSV files into one DataFrame\n",
        "all_data = pd.DataFrame()\n",
        "\n",
        "for file_name in os.listdir(folder_path):\n",
        "    if file_name.endswith('.csv'):  # Ensure it's a CSV file\n",
        "        file_path = os.path.join(folder_path, file_name)\n",
        "        # Specify the semicolon as the delimiter\n",
        "        temp_data = pd.read_csv(file_path, sep=';')\n",
        "        all_data = pd.concat([all_data, temp_data], ignore_index=True)\n",
        "\n",
        "# Display the combined dataset\n",
        "print(\"Combined DataFrame:\")\n",
        "print(all_data.info())\n",
        "print(all_data.head())\n"
      ],
      "metadata": {
        "id": "MK8MsP8rMfE7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load one of the CSV files for inspection\n",
        "file_path = '/content/drive/My Drive/Preprocessed/HUPA0015P.csv'  # Replace with the actual path\n",
        "data = pd.read_csv(file_path, sep=';', header=None)  # Use sep=';' to handle semicolon delimiter\n",
        "\n",
        "# Display the first few rows\n",
        "print(data.head())"
      ],
      "metadata": {
        "id": "2GUiw3XNMhoq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(file_path, sep=';', header=0, engine='python')  # Use 'python' engine for better compatibility\n",
        "print(data.columns)\n",
        "print(data.head())\n"
      ],
      "metadata": {
        "id": "wxwG-il4MjX5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Path to the folder\n",
        "folder_path = '/content/drive/My Drive/Preprocessed'\n",
        "\n",
        "all_data = pd.DataFrame()\n",
        "\n",
        "for file_name in os.listdir(folder_path):\n",
        "    if file_name.endswith('.csv'):\n",
        "        file_path = os.path.join(folder_path, file_name)\n",
        "\n",
        "        # Read and split the data\n",
        "        temp_data = pd.read_csv(file_path, header=None)\n",
        "        temp_data_split = temp_data[0].str.split(';', expand=True)\n",
        "        temp_data_split.columns = ['time', 'glucose', 'calories', 'heart_rate', 'steps', 'basal_rate', 'bolus_volume_delivered', 'carb_input']\n",
        "\n",
        "        # Combine into the main DataFrame\n",
        "        all_data = pd.concat([all_data, temp_data_split], ignore_index=True)\n",
        "\n",
        "print(all_data.info())\n",
        "print(all_data.head())\n"
      ],
      "metadata": {
        "id": "aUsLTBKSMlJC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import numpy as np\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, confusion_matrix, precision_score, recall_score, accuracy_score, f1_score\n",
        "\n",
        "\n",
        "# Combine all preprocessed CSV files\n",
        "preprocessed_dir = '/content/drive/My Drive/Preprocessed'\n",
        "all_data = pd.DataFrame()\n",
        "\n",
        "for file_name in os.listdir(preprocessed_dir):\n",
        "    file_path = os.path.join(preprocessed_dir, file_name)\n",
        "    # Read and split the data, assuming semicolon delimiter and no header\n",
        "    temp_data = pd.read_csv(file_path, header=None)\n",
        "    temp_data_split = temp_data[0].str.split(';', expand=True)\n",
        "    temp_data_split.columns = ['time', 'glucose', 'calories', 'heart_rate', 'steps', 'basal_rate', 'bolus_volume_delivered', 'carb_input']\n",
        "    all_data = pd.concat([all_data, temp_data_split], ignore_index=True)\n",
        "\n",
        "# Separate features and target\n",
        "target_column = 'glucose'\n",
        "X = all_data.drop(columns=[target_column, 'time'])\n",
        "y = all_data[target_column]\n",
        "\n",
        "# Convert columns to numeric, handling errors\n",
        "X = X.apply(pd.to_numeric, errors='coerce').fillna(0)  # Replace non-numeric with 0\n",
        "y = pd.to_numeric(y, errors='coerce').fillna(0)  # Replace non-numeric with 0\n",
        "\n",
        "# Normalize features\n",
        "scaler = MinMaxScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "# --- BRC Model ---\n",
        "# Create random reservoirs for forward and backward directions\n",
        "reservoir_size = 500\n",
        "reservoir_forward = np.random.rand(X_train.shape[1], reservoir_size) * 2 - 1\n",
        "reservoir_backward = np.random.rand(X_train.shape[1], reservoir_size) * 2 - 1\n",
        "\n",
        "# Transform training and testing data using the forward reservoir\n",
        "reservoir_train_forward = np.tanh(np.dot(X_train, reservoir_forward))\n",
        "reservoir_test_forward = np.tanh(np.dot(X_test, reservoir_forward))\n",
        "\n",
        "# Reverse the input data for the backward reservoir\n",
        "X_train_reversed = np.flip(X_train, axis=0)\n",
        "X_test_reversed = np.flip(X_test, axis=0)\n",
        "\n",
        "# Transform reversed data using the backward reservoir\n",
        "reservoir_train_backward = np.tanh(np.dot(X_train_reversed, reservoir_backward))\n",
        "reservoir_test_backward = np.tanh(np.dot(X_test_reversed, reservoir_backward))\n",
        "\n",
        "# Flip the backward states back to the original order\n",
        "reservoir_train_backward = np.flip(reservoir_train_backward, axis=0)\n",
        "reservoir_test_backward = np.flip(reservoir_test_backward, axis=0)\n",
        "\n",
        "# Combine forward and backward reservoir states\n",
        "reservoir_train_combined = np.hstack((reservoir_train_forward, reservoir_train_backward))\n",
        "reservoir_test_combined = np.hstack((reservoir_test_forward, reservoir_test_backward))\n",
        "\n",
        "# Handle NaN values in combined reservoir states and target (if any)\n",
        "reservoir_train_combined = np.nan_to_num(reservoir_train_combined)\n",
        "reservoir_test_combined = np.nan_to_num(reservoir_test_combined)\n",
        "\n",
        "# Train the readout layer using Ridge Regression\n",
        "ridge = Ridge(alpha=1.0)\n",
        "ridge.fit(reservoir_train_combined, y_train)\n",
        "\n",
        "# Evaluate and calculate metrics\n",
        "predictions_brc = ridge.predict(reservoir_test_combined)\n",
        "\n",
        "# --- Regression Metrics ---\n",
        "mse_brc = mean_squared_error(y_test, predictions_brc)\n",
        "mae_brc = mean_absolute_error(y_test, predictions_brc)\n",
        "r2_brc = r2_score(y_test, predictions_brc)\n",
        "\n",
        "print(\"BRC Regression Metrics:\")\n",
        "print(f\"Mean Squared Error (MSE): {mse_brc}\")\n",
        "print(f\"Mean Absolute Error (MAE): {mae_brc}\")\n",
        "print(f\"R-squared (R2): {r2_brc}\")\n",
        "\n",
        "# --- Categorization for Classification Metrics ---\n",
        "thresholds = [70, 140]  # Example thresholds\n",
        "y_test_cat = np.digitize(y_test, thresholds)\n",
        "predictions_brc_cat = np.digitize(predictions_brc, thresholds)\n",
        "\n",
        "# --- Classification Metrics ---\n",
        "cm_brc = confusion_matrix(y_test_cat, predictions_brc_cat)\n",
        "precision_brc = precision_score(y_test_cat, predictions_brc_cat, average='weighted')\n",
        "recall_brc = recall_score(y_test_cat, predictions_brc_cat, average='weighted')\n",
        "accuracy_brc = accuracy_score(y_test_cat, predictions_brc_cat)\n",
        "f1_brc = f1_score(y_test_cat, predictions_brc_cat, average='weighted')\n",
        "\n",
        "print(\"\\nBRC Classification Metrics:\")\n",
        "print(\"Confusion Matrix:\", cm_brc)\n",
        "print(\"Precision:\", precision_brc)\n",
        "print(\"Recall:\", recall_brc)\n",
        "print(\"Accuracy:\", accuracy_brc)\n",
        "print(\"F1-score:\", f1_brc)"
      ],
      "metadata": {
        "id": "3lpZyiS-Mr3q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}